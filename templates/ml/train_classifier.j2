{#
Template: Classification Model Training
Description: Train classification models with cross-validation and metrics
Author: System
Version: 1.0
Required_params: target_column, features, model_type
#}
#!/usr/bin/env python3
"""
Generated script for classification model training operation.
Generated at: {{ timestamp }}
"""

import json
import sys
import pandas as pd
import numpy as np
from scipy import stats
from typing import Any, Dict, List, Optional, Tuple

def main() -> None:
    """Main execution function."""
    try:
        # Read input data from stdin
        input_data = json.loads(sys.stdin.read())
        df = pd.DataFrame(input_data['dataframe'])
        params = input_data.get('parameters', {})

        # Validate input data
        validate_input(df, params)

        # Execute model training
        result = train_classification_model(df, params)

        # Output results as JSON
        output = {
            'success': True,
            'result': result,
            'metadata': {
                'operation': 'train_classifier',
                'timestamp': '{{ timestamp }}',
                'rows_processed': len(df),
                'model_type': params.get('model_type', 'logistic_regression')
            }
        }

        print(json.dumps(output, default=str))

    except Exception as e:
        error_output = {
            'success': False,
            'error': str(e),
            'error_type': type(e).__name__,
            'operation': 'train_classifier',
            'timestamp': '{{ timestamp }}'
        }
        print(json.dumps(error_output))
        return  # Exit gracefully without sys.exit

def validate_input(df: pd.DataFrame, params: dict) -> None:
    """Validate input data and parameters."""
    if df.empty:
        raise ValueError("Input dataframe is empty")

    # Check target column
    target_column = params.get('target_column')
    if not target_column:
        raise ValueError("target_column parameter is required")

    if target_column not in df.columns:
        raise ValueError(f"Target column '{target_column}' not found in dataframe")

    # Check features
    features = params.get('features')
    if not features:
        raise ValueError("features parameter is required")

    missing_features = [f for f in features if f not in df.columns]
    if missing_features:
        raise ValueError(f"Feature columns not found: {missing_features}")

    # Validate model type
    model_type = params.get('model_type', 'logistic_regression')
    valid_models = ['logistic_regression', 'decision_tree', 'random_forest', 'svm', 'naive_bayes']
    if model_type not in valid_models:
        raise ValueError(f"Invalid model type '{model_type}'. Must be one of: {valid_models}")

def train_classification_model(df: pd.DataFrame, params: dict) -> dict:
    """Train a classification model and return results."""

    target_column = params['target_column']
    features = params['features']
    model_type = params.get('model_type', 'logistic_regression')
    test_size = params.get('test_size', 0.2)
    random_state = params.get('random_state', 42)
    cv_folds = params.get('cv_folds', 5)

    # Prepare data
    X = df[features].copy()
    y = df[target_column].copy()

    # Handle missing values
    X_clean, y_clean = handle_missing_values(X, y)

    if len(X_clean) == 0:
        raise ValueError("No samples remain after handling missing values")

    # Check if we have enough data
    unique_labels = np.unique(y_clean)
    if len(unique_labels) < 2:
        raise ValueError("Classification requires at least 2 unique target values")

    min_class_count = min(np.bincount(y_clean.astype(int) if y_clean.dtype.kind in 'biufc' else
                                     pd.Categorical(y_clean).codes))
    if min_class_count < 2:
        raise ValueError("Each class must have at least 2 samples")

    # Encode features and target if needed
    X_encoded, feature_encoders = encode_features(X_clean)
    y_encoded, label_encoder = encode_target(y_clean)

    # Split data
    train_indices, test_indices = train_test_split_simple(len(X_encoded), test_size, random_state)

    X_train = X_encoded.iloc[train_indices]
    X_test = X_encoded.iloc[test_indices]
    y_train = y_encoded[train_indices]
    y_test = y_encoded[test_indices]

    # Scale features
    X_train_scaled, X_test_scaled, scaler_params = scale_features(X_train, X_test)

    # Train model
    model_params = train_model(X_train_scaled, y_train, model_type, random_state)

    # Make predictions
    y_pred_train = predict_model(X_train_scaled, model_params, model_type)
    y_pred_test = predict_model(X_test_scaled, model_params, model_type)

    # Calculate metrics
    train_metrics = calculate_classification_metrics(y_train, y_pred_train)
    test_metrics = calculate_classification_metrics(y_test, y_pred_test)

    # Cross-validation
    cv_scores = cross_validate_model(X_encoded, y_encoded, model_type, cv_folds, random_state)

    # Feature importance (if applicable)
    feature_importance = get_feature_importance(model_params, model_type, features)

    # Prepare results
    result = {
        'model_info': {
            'model_type': model_type,
            'feature_count': len(features),
            'features': features,
            'target_column': target_column,
            'training_samples': len(X_train),
            'test_samples': len(X_test),
            'unique_classes': len(unique_labels),
            'class_distribution': get_class_distribution(y_clean)
        },
        'performance': {
            'train_metrics': train_metrics,
            'test_metrics': test_metrics,
            'cross_validation': {
                'mean_accuracy': float(np.mean(cv_scores)),
                'std_accuracy': float(np.std(cv_scores)),
                'scores': [float(s) for s in cv_scores]
            }
        },
        'model_analysis': {
            'feature_importance': feature_importance,
            'overfitting_check': {
                'train_accuracy': train_metrics['accuracy'],
                'test_accuracy': test_metrics['accuracy'],
                'difference': train_metrics['accuracy'] - test_metrics['accuracy'],
                'likely_overfitting': (train_metrics['accuracy'] - test_metrics['accuracy']) > 0.1
            }
        },
        'data_preprocessing': {
            'missing_values_handled': params.get('handle_missing', True),
            'features_scaled': True,
            'categorical_encoded': len(feature_encoders) > 0,
            'original_samples': len(df),
            'final_samples': len(X_clean)
        }
    }

    return result

def handle_missing_values(X: pd.DataFrame, y: pd.Series) -> Tuple[pd.DataFrame, pd.Series]:
    """Handle missing values by removing rows with any missing data."""
    # Simple approach: remove rows with any missing values
    mask = ~(X.isnull().any(axis=1) | y.isnull())
    return X[mask].reset_index(drop=True), y[mask].reset_index(drop=True)

def encode_features(X: pd.DataFrame) -> Tuple[pd.DataFrame, dict]:
    """Encode categorical features to numeric."""
    X_encoded = X.copy()
    encoders = {}

    for col in X.columns:
        if X[col].dtype == 'object' or X[col].dtype.name == 'category':
            # Simple label encoding for categorical variables
            unique_values = X[col].unique()
            encoding_map = {val: idx for idx, val in enumerate(unique_values)}
            X_encoded[col] = X[col].map(encoding_map)
            encoders[col] = encoding_map

    return X_encoded, encoders

def encode_target(y: pd.Series) -> Tuple[np.ndarray, Optional[dict]]:
    """Encode target variable if it's categorical."""
    if y.dtype == 'object' or y.dtype.name == 'category':
        unique_values = y.unique()
        encoding_map = {val: idx for idx, val in enumerate(unique_values)}
        y_encoded = y.map(encoding_map).values
        return y_encoded, encoding_map
    else:
        return y.values, None

def train_test_split_simple(n_samples: int, test_size: float, random_state: int) -> Tuple[List[int], List[int]]:
    """Simple train-test split implementation."""
    np.random.seed(random_state)
    indices = np.random.permutation(n_samples)
    test_samples = int(n_samples * test_size)

    test_indices = indices[:test_samples].tolist()
    train_indices = indices[test_samples:].tolist()

    return train_indices, test_indices

def scale_features(X_train: pd.DataFrame, X_test: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, dict]:
    """Scale features using standardization."""
    means = X_train.mean()
    stds = X_train.std()

    # Replace zero std with 1 to avoid division by zero
    stds = stds.replace(0, 1)

    X_train_scaled = (X_train - means) / stds
    X_test_scaled = (X_test - means) / stds

    scaler_params = {'means': means.to_dict(), 'stds': stds.to_dict()}

    return X_train_scaled, X_test_scaled, scaler_params

def train_model(X: pd.DataFrame, y: np.ndarray, model_type: str, random_state: int) -> dict:
    """Train model and return parameters."""

    if model_type == 'logistic_regression':
        return train_logistic_regression(X, y, random_state)
    elif model_type == 'decision_tree':
        return train_decision_tree(X, y, random_state)
    elif model_type == 'naive_bayes':
        return train_naive_bayes(X, y)
    else:
        # For unsupported models, use logistic regression as fallback
        return train_logistic_regression(X, y, random_state)

def train_logistic_regression(X: pd.DataFrame, y: np.ndarray, random_state: int) -> dict:
    """Train logistic regression using Newton's method approximation."""
    X_array = X.values

    # Add intercept term
    X_with_intercept = np.column_stack([np.ones(X_array.shape[0]), X_array])

    # Initialize weights
    np.random.seed(random_state)
    weights = np.random.normal(0, 0.01, X_with_intercept.shape[1])

    # Simple gradient descent
    learning_rate = 0.01
    max_iterations = 1000

    for _ in range(max_iterations):
        # Predictions
        z = X_with_intercept @ weights
        predictions = sigmoid(z)

        # Gradient
        gradient = X_with_intercept.T @ (predictions - y) / len(y)

        # Update weights
        weights -= learning_rate * gradient

        # Check convergence (simple)
        if np.max(np.abs(gradient)) < 1e-6:
            break

    return {
        'model_type': 'logistic_regression',
        'weights': weights.tolist(),
        'feature_names': ['intercept'] + list(X.columns)
    }

def train_decision_tree(X: pd.DataFrame, y: np.ndarray, random_state: int) -> dict:
    """Train a simple decision tree (stub implementation)."""
    # Simplified decision tree - just store means for prediction
    feature_means = X.mean().to_dict()
    class_majority = int(stats.mode(y)[0])

    return {
        'model_type': 'decision_tree',
        'feature_means': feature_means,
        'majority_class': class_majority,
        'feature_names': list(X.columns)
    }

def train_naive_bayes(X: pd.DataFrame, y: np.ndarray) -> dict:
    """Train Gaussian Naive Bayes."""
    classes = np.unique(y)
    class_stats = {}

    for cls in classes:
        mask = y == cls
        X_class = X[mask]

        class_stats[int(cls)] = {
            'prior': float(np.mean(mask)),
            'means': X_class.mean().to_dict(),
            'stds': X_class.std().to_dict()
        }

    return {
        'model_type': 'naive_bayes',
        'class_stats': class_stats,
        'feature_names': list(X.columns)
    }

def sigmoid(z: np.ndarray) -> np.ndarray:
    """Sigmoid activation function."""
    return 1 / (1 + np.exp(-np.clip(z, -250, 250)))  # Clip to prevent overflow

def predict_model(X: pd.DataFrame, model_params: dict, model_type: str) -> np.ndarray:
    """Make predictions using trained model."""

    if model_type == 'logistic_regression':
        return predict_logistic_regression(X, model_params)
    elif model_type == 'decision_tree':
        return predict_decision_tree(X, model_params)
    elif model_type == 'naive_bayes':
        return predict_naive_bayes(X, model_params)
    else:
        return predict_logistic_regression(X, model_params)

def predict_logistic_regression(X: pd.DataFrame, model_params: dict) -> np.ndarray:
    """Make predictions with logistic regression."""
    X_array = X.values
    X_with_intercept = np.column_stack([np.ones(X_array.shape[0]), X_array])
    weights = np.array(model_params['weights'])

    z = X_with_intercept @ weights
    probabilities = sigmoid(z)

    return (probabilities > 0.5).astype(int)

def predict_decision_tree(X: pd.DataFrame, model_params: dict) -> np.ndarray:
    """Make predictions with decision tree (simplified)."""
    # Simplified prediction based on feature means
    majority_class = model_params['majority_class']
    return np.full(len(X), majority_class)

def predict_naive_bayes(X: pd.DataFrame, model_params: dict) -> np.ndarray:
    """Make predictions with Naive Bayes."""
    class_stats = model_params['class_stats']
    classes = list(class_stats.keys())

    predictions = []

    for _, row in X.iterrows():
        class_probs = {}

        for cls in classes:
            log_prob = np.log(class_stats[cls]['prior'])

            for feature in X.columns:
                mean = class_stats[cls]['means'][feature]
                std = class_stats[cls]['stds'][feature]

                if std > 0:
                    # Gaussian probability
                    log_prob += -0.5 * np.log(2 * np.pi * std**2)
                    log_prob += -0.5 * ((row[feature] - mean)**2) / (std**2)

            class_probs[cls] = log_prob

        predicted_class = max(class_probs, key=class_probs.get)
        predictions.append(predicted_class)

    return np.array(predictions)

def calculate_classification_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> dict:
    """Calculate classification metrics."""
    accuracy = np.mean(y_true == y_pred)

    # For binary classification, calculate additional metrics
    if len(np.unique(y_true)) == 2:
        tp = np.sum((y_true == 1) & (y_pred == 1))
        tn = np.sum((y_true == 0) & (y_pred == 0))
        fp = np.sum((y_true == 0) & (y_pred == 1))
        fn = np.sum((y_true == 1) & (y_pred == 0))

        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

        return {
            'accuracy': float(accuracy),
            'precision': float(precision),
            'recall': float(recall),
            'f1_score': float(f1),
            'confusion_matrix': {
                'true_positive': int(tp),
                'true_negative': int(tn),
                'false_positive': int(fp),
                'false_negative': int(fn)
            }
        }
    else:
        return {
            'accuracy': float(accuracy),
            'note': 'Additional metrics available for binary classification only'
        }

def cross_validate_model(X: pd.DataFrame, y: np.ndarray, model_type: str, cv_folds: int, random_state: int) -> np.ndarray:
    """Perform cross-validation."""
    np.random.seed(random_state)
    n_samples = len(X)
    indices = np.random.permutation(n_samples)
    fold_size = n_samples // cv_folds

    scores = []

    for i in range(cv_folds):
        start = i * fold_size
        end = start + fold_size if i < cv_folds - 1 else n_samples

        test_indices = indices[start:end]
        train_indices = np.concatenate([indices[:start], indices[end:]])

        X_train_cv = X.iloc[train_indices]
        X_test_cv = X.iloc[test_indices]
        y_train_cv = y[train_indices]
        y_test_cv = y[test_indices]

        # Scale features
        X_train_scaled, X_test_scaled, _ = scale_features(X_train_cv, X_test_cv)

        # Train and predict
        model_params = train_model(X_train_scaled, y_train_cv, model_type, random_state)
        y_pred_cv = predict_model(X_test_scaled, model_params, model_type)

        # Calculate accuracy
        accuracy = np.mean(y_test_cv == y_pred_cv)
        scores.append(accuracy)

    return np.array(scores)

def get_feature_importance(model_params: dict, model_type: str, feature_names: List[str]) -> dict:
    """Get feature importance scores."""

    if model_type == 'logistic_regression':
        weights = np.array(model_params['weights'][1:])  # Exclude intercept
        importance_scores = np.abs(weights)

        # Normalize to sum to 1
        if np.sum(importance_scores) > 0:
            importance_scores = importance_scores / np.sum(importance_scores)

        return {
            feature: float(score)
            for feature, score in zip(feature_names, importance_scores)
        }

    else:
        # For other models, return equal importance
        equal_importance = 1.0 / len(feature_names)
        return {feature: equal_importance for feature in feature_names}

def get_class_distribution(y: pd.Series) -> dict:
    """Get class distribution statistics."""
    counts = y.value_counts()
    total = len(y)

    return {
        'counts': counts.to_dict(),
        'percentages': (counts / total * 100).to_dict(),
        'most_common': str(counts.index[0]),
        'least_common': str(counts.index[-1])
    }

if __name__ == "__main__":
    main()